{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3GB3qXLRYmfz6W2aaGDe6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Practical 6 : Create a Neural network architecture from scratch in Python and use it to do multi-class classification on any data"],"metadata":{"id":"WSCAYy0bMLg-"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Activation Functions\n","def softmax(z):\n","    exp = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n","    return exp / np.sum(exp, axis=1, keepdims=True)\n","\n","def relu(z):\n","    return np.maximum(0, z)\n","\n","def relu_derivative(z):\n","    return (z > 0).astype(float)\n","\n","# Loss Function\n","def cross_entropy(y_true, y_pred):\n","    m = y_true.shape[0]\n","    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n","    return loss\n","\n","# Neural Network Class\n","class NeuralNetwork:\n","    def __init__(self, input_size, hidden_size, output_size):\n","        # Xavier Initialization\n","        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n","        self.b1 = np.zeros((1, hidden_size))\n","        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n","        self.b2 = np.zeros((1, output_size))\n","\n","    def forward(self, X):\n","        self.Z1 = X @ self.W1 + self.b1\n","        self.A1 = relu(self.Z1)\n","        self.Z2 = self.A1 @ self.W2 + self.b2\n","        self.A2 = softmax(self.Z2)\n","        return self.A2\n","\n","    def backward(self, X, y_true, learning_rate=0.01):\n","        m = X.shape[0]\n","\n","        dZ2 = self.A2 - y_true\n","        dW2 = (self.A1.T @ dZ2) / m\n","        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n","\n","        dA1 = dZ2 @ self.W2.T\n","        dZ1 = dA1 * relu_derivative(self.Z1)\n","        dW1 = (X.T @ dZ1) / m\n","        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n","\n","        # Update weights and biases\n","        self.W1 -= learning_rate * dW1\n","        self.b1 -= learning_rate * db1\n","        self.W2 -= learning_rate * dW2\n","        self.b2 -= learning_rate * db2\n","\n","    def train(self, X, y, epochs=1000, learning_rate=0.01):\n","        for i in range(epochs):\n","            y_pred = self.forward(X)\n","            loss = cross_entropy(y, y_pred)\n","            self.backward(X, y, learning_rate)\n","\n","            if i % 100 == 0:\n","                print(f\"Epoch {i}, Loss: {loss:.4f}\")\n","\n","    def predict(self, X):\n","        y_pred = self.forward(X)\n","        return np.argmax(y_pred, axis=1)\n","\n","# Load Dataset\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target.reshape(-1, 1)\n","\n","# One hot encoding the target\n","encoder = OneHotEncoder(sparse_output=False)\n","y_encoded = encoder.fit_transform(y)\n","\n","# Train/Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n","\n","# Build and Train Model\n","input_size = X_train.shape[1]\n","hidden_size = 10\n","output_size = y_train.shape[1]\n","\n","nn = NeuralNetwork(input_size, hidden_size, output_size)\n","nn.train(X_train, y_train, epochs=1000, learning_rate=0.01)\n","\n","# Evaluate\n","y_pred_test = nn.predict(X_test)\n","y_true_test = np.argmax(y_test, axis=1)\n","\n","accuracy = np.mean(y_pred_test == y_true_test)\n","print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mX_oR_cqNDHS","executionInfo":{"status":"ok","timestamp":1745739874085,"user_tz":-330,"elapsed":4794,"user":{"displayName":"Abizer Lunawadawala","userId":"10706016738865616886"}},"outputId":"ee4ec873-e35a-411e-c8bb-fa44a8560f1f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 1.6441\n","Epoch 100, Loss: 0.6327\n","Epoch 200, Loss: 0.4530\n","Epoch 300, Loss: 0.3753\n","Epoch 400, Loss: 0.3233\n","Epoch 500, Loss: 0.2827\n","Epoch 600, Loss: 0.2498\n","Epoch 700, Loss: 0.2226\n","Epoch 800, Loss: 0.2003\n","Epoch 900, Loss: 0.1820\n","Test Accuracy: 96.67%\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTEtdqf-MICf"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","# Add header names\n","headers = ['age', 'sex', 'chest_pain', 'resting_blood_pressure', 'serum_cholestoral',\n","'fasting_blood_sugar', 'resting_ecg_results', 'max_heart_rate_achieved',\n","'exercise_induced_angina', 'oldpeak', 'slope_of_the_peak', 'num_of_major_vessels',\n","'thal', 'heart_disease']\n","# Load the dataset\n","heart_df = pd.read_csv('/home/student/Downloads/heart.csv', sep=',', names=headers)\n","# Convert input to numeric values (if any non-numeric values exist)\n","heart_df = heart_df.apply(pd.to_numeric, errors='coerce')\n","# Handle missing values (if any)\n","heart_df.fillna(heart_df.mean(), inplace=True)\n","# Convert input data to numpy arrays\n","X = heart_df.drop(columns=['heart_disease'])\n","# Replace target class with 0 and 1\n","heart_df['heart_disease'] = heart_df['heart_disease'].replace({1: 0, 2: 1})\n","y_label = heart_df['heart_disease'].values.reshape(-1, 1)\n","# Split data into train and test sets\n","Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size=0.2, random_state=2)\n","# Standardize the dataset\n","sc = StandardScaler()\n","Xtrain = sc.fit_transform(Xtrain)\n","Xtest = sc.transform(Xtest)\n","print(heart_df.dtypes)\n","print(f\"\\nShape of train set is {Xtrain.shape}\")\n","print(f\"\\nShape of test set is {Xtest.shape}\")\n","print(f\"\\nShape of train label is {ytrain.shape}\")\n","print(f\"\\nShape of test labels is {ytest.shape}\")\n","# Neural Network Class\n","class NeuralNet:\n","def _init_(self, layers=[13, 8, 1], learning_rate=0.001, iterations=1000):\n","self.params = {}\n","self.learning_rate = learning_rate\n","self.iterations = iterations\n","self.loss = []\n","self.layers = layers\n","self.X = None\n","self.y = None\n","def init_weights(self):\n","np.random.seed(1)\n","self.params[\"W1\"] = np.random.randn(self.layers[0], self.layers[1]) * 0.01\n","self.params['b1'] = np.zeros((1, self.layers[1]))\n","self.params['W2'] = np.random.randn(self.layers[1], self.layers[2]) * 0.01\n","self.params['b2'] = np.zeros((1, self.layers[2]))\n","def sigmoid(self, Z):\n","return 1 / (1 + np.exp(-Z))\n","def dSigmoid(self, Z):\n","sig = self.sigmoid(Z)\n","return sig * (1 - sig)\n","def relu(self, Z):\n","return np.maximum(0, Z)\n","def dRelu(self, Z):\n","return (Z > 0).astype(float)\n","def eta(self, x):\n","return np.maximum(x, 1e-10)\n","def entropy_loss(self, y, yhat):\n","epsilon = 1e-10\n","yhat = np.clip(yhat, epsilon, 1 - epsilon)\n","loss = -np.mean(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))\n","return loss\n","def forward_propagation(self):\n","Z1 = self.X.dot(self.params['W1']) + self.params['b1']\n","A1 = self.relu(Z1)\n","Z2 = A1.dot(self.params['W2']) + self.params['b2']\n","yhat = self.sigmoid(Z2)\n","loss = self.entropy_loss(self.y, yhat)\n","self.params['Z1'] = Z1\n","self.params['Z2'] = Z2\n","self.params['A1'] = A1\n","return yhat, loss\n","def back_propagation(self, yhat):\n","m = self.X.shape[0]\n","y_inv = 1 - self.y\n","yhat_inv = 1 - yhat\n","dl_wrt_yhat = (yhat - self.y) / m\n","dl_wrt_sig = self.dSigmoid(self.params['Z2'])\n","dl_wrt_z2 = dl_wrt_yhat * dl_wrt_sig\n","dl_wrt_A1 = dl_wrt_z2.dot(self.params['W2'].T)\n","dl_wrt_w2 = self.params['A1'].T.dot(dl_wrt_z2)\n","dl_wrt_b2 = np.sum(dl_wrt_z2, axis=0, keepdims=True)\n","dl_wrt_z1 = dl_wrt_A1 * self.dRelu(self.params['Z1'])\n","dl_wrt_w1 = self.X.T.dot(dl_wrt_z1)\n","dl_wrt_b1 = np.sum(dl_wrt_z1, axis=0, keepdims=True)\n","self.params['W1'] -= self.learning_rate * dl_wrt_w1\n","self.params['W2'] -= self.learning_rate * dl_wrt_w2\n","self.params['b1'] -= self.learning_rate * dl_wrt_b1\n","self.params['b2'] -= self.learning_rate * dl_wrt_b2\n","def fit(self, X, y):\n","self.X = X\n","self.y = y\n","self.init_weights()\n","for i in range(self.iterations):\n","yhat, loss = self.forward_propagation()\n","self.back_propagation(yhat)\n","self.loss.append(loss)\n","# Learning rate decay (optional)\n","if i % 100 == 0 and i != 0:\n","self.learning_rate *= 0.9 # Decay learning rate\n","if i % 100 == 0:\n","print(f\"Iteration {i}, Loss: {loss}\")\n","def predict(self, X):\n","Z1 = X.dot(self.params['W1']) + self.params['b1']\n","A1 = self.relu(Z1)\n","Z2 = A1.dot(self.params['W2']) + self.params['b2']\n","yhat = self.sigmoid(Z2)\n","return np.round(yhat)\n","def acc(self, y, yhat):\n","return np.mean(y == yhat) * 100\n","def plot_loss(self):\n","plt.plot(self.loss)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss curve during training\")\n","plt.show()\n","# Train the model\n","nn = NeuralNet(layers=[13, 8, 1], learning_rate=0.001, iterations=1000)\n","nn.fit(Xtrain, ytrain)\n","# Evaluate the model\n","y_pred = nn.predict(Xtest)\n","accuracy = nn.acc(ytest, y_pred)\n","print(f\"Test Accuracy: {accuracy}%\")\n","# Plot the loss curve\n","nn.plot_loss()"]}]}